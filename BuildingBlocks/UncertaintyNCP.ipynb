{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reliable Uncertainty Estimates in Deep Neural Networks using Noise Contrastive Priors\n",
    "\n",
    "PyTorch implementation of [\"Reliable Uncertainty Estimates in Deep Neural Networks using Noise Contrastive Priors\"](https://arxiv.org/abs/1807.09289). It is based on the [reference tensorflow implementation](https://github.com/brain-research/ncp). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    w_{ij} &= \\bar{w}_{ij} + \\sigma_{w,ij} \\epsilon_{w,ij} \\\\\n",
    "    b_i &= \\bar{b}_i + \\sigma_{b,i} \\epsilon_{b,i}\n",
    "    \\mu_i &= \\sum_i w_{ij} x_j + b_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\langle \\epsilon_{ij} \\epsilon_{kl} \\rangle &= \\delta_{ik} \\delta{jl} \\\\\n",
    "    \\langle \\epsilon_{b,i} \\epsilon_{b,k} \\rangle  &= \\delta_{ik}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\langle \\mu_i \\rangle &= \\sum_j \\bar{w}_{ij} x_j + \\bar{b}_i \\\\\n",
    "    \\langle (\\mu_i - \\bar{\\mu}_i) (\\mu_k - \\bar{\\mu}_k) \\rangle \n",
    "        &= \n",
    "            \\sum_{jl} \\sigma_{w,ij} x_j \\sigma_{w,kl} x_l \\langle \\epsilon_{w,ij} \\epsilon_{w,kl} \\rangle  \n",
    "            + \\sigma_{b,i} \\sigma_{b,k} \\langle \\epsilon_{b,i} \\epsilon_{b,k} \\rangle  \n",
    "            \\\\\n",
    "        &= \\delta_{ik} \\left( \\sum_{j} \\sigma_{w,ij}^2 x_j^2 + \\sigma_i^2 \\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "from chmp.ds import get_color_cycle, Loop\n",
    "from chmp.torch_utils.nn import Lambda, t2n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(length=1000, noise_slope=0.2):\n",
    "    \"Adapted from https://github.com/brain-research/ncp/blob/master/ncp/datasets/toy.py\"\n",
    "    random = np.random.RandomState(0)\n",
    "    \n",
    "    inputs = np.linspace(-1, 1, length)\n",
    "    noise_std = np.maximum(0, (inputs + 1) * noise_slope)\n",
    "    targets = 0.5 * + np.sin(25 * inputs) + random.normal(0, noise_std)\n",
    "    targets += 0.5 * inputs\n",
    "    \n",
    "    domain = np.linspace(-1.2, 1.2, 1000)\n",
    "    train_split = np.repeat([False, True, False, True, False], 200)\n",
    "    test_split = (1 - train_split).astype(bool)\n",
    "    domain, inputs, targets = domain[:, None], inputs[:, None], targets[:, None]\n",
    "    test_inputs, test_targets = inputs[test_split], targets[test_split]\n",
    "    train_inputs, train_targets = inputs[train_split], targets[train_split]\n",
    "    \n",
    "    return dict(\n",
    "        domain=domain, \n",
    "        target_scale=1,\n",
    "        train=dict(inputs=train_inputs, targets=train_targets),\n",
    "        test=dict(inputs=test_inputs, targets=test_targets),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0, c1, c2 = get_color_cycle(3)\n",
    "\n",
    "for label, c in [('train', c1), ('test', c2)]:\n",
    "    plt.plot(data[label]['inputs'][:, 0], data[label]['targets'][:, 0], '.', alpha=0.2, c=c, label=label)\n",
    "    \n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCPEstimator(torch.nn.Module):\n",
    "    def __init__(self, transform, transform_features, out_features, prior_scale=1e-1, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.prior_scale = prior_scale\n",
    "        self.transform_features = transform_features\n",
    "        self.out_features = out_features\n",
    "        self.eps = eps\n",
    "        \n",
    "        _p = torch.nn.Parameter\n",
    "        \n",
    "        self.mean_weight_loc = _p(torch.empty(self.out_features, self.transform_features))\n",
    "        self.mean_weight_scale_p = _p(torch.ones(self.out_features, self.transform_features))\n",
    "        \n",
    "        self.mean_bias_loc = _p(torch.empty(self.out_features))\n",
    "        self.mean_bias_scale_p = _p(torch.ones(self.out_features))\n",
    "        \n",
    "        self.to_scale = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.transform_features, self.out_features),\n",
    "            Lambda(lambda x: eps + F.softplus(x)),\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            torch.nn.init.xavier_uniform_(self.mean_weight_loc)\n",
    "            torch.nn.init.uniform_(self.mean_bias_loc, -1e-4, +1e-4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Return the mean distribution and the target distribution.\"\"\"\n",
    "        hidden = self.transform(x)\n",
    "        \n",
    "        mean_weight_scale = self.eps + F.softplus(self.mean_weight_scale_p)\n",
    "        mean_bias_scale = self.eps + F.softplus(self.mean_bias_scale_p)\n",
    "        \n",
    "        mean_loc = F.linear(hidden, self.mean_weight_loc, self.mean_bias_loc)\n",
    "        mean_var = F.linear(hidden ** 2.0, mean_weight_scale ** 2.0, mean_bias_scale ** 2.0)\n",
    "        mean_scale = torch.sqrt(mean_var)\n",
    "        \n",
    "        mean_params = mean_loc, mean_scale\n",
    "        \n",
    "        weight = self.mean_weight_loc + torch.randn_like(mean_weight_scale) * mean_weight_scale\n",
    "        bias = self.mean_bias_loc + torch.randn_like(mean_bias_scale) * mean_bias_scale\n",
    "        \n",
    "        target_loc = F.linear(hidden, weight, bias)\n",
    "        target_scale = self.to_scale(hidden)\n",
    "        \n",
    "        target_params = target_loc, target_scale\n",
    "        \n",
    "        return target_params, mean_params\n",
    "    \n",
    "    def predict(self, x):\n",
    "        (_, target_scale), (mean_loc, mean_scale) = self(x)\n",
    "        return mean_loc, target_scale, mean_scale\n",
    "    \n",
    "    def loss(self, x, y, input_noise, ood_mean_std=1.0, n_samples=1.0, bbb_scale=1.0, ncp_scale=1.0):\n",
    "        ood_x = x + input_noise * torch.randn_like(x)\n",
    "        \n",
    "        (target_loc, target_scale), (mean_loc, mean_scale) = self(x)\n",
    "        _, (ood_mean_loc, ood_mean_scale) = self(ood_x)\n",
    "        \n",
    "        nll = -torch.distributions.Normal(target_loc, target_scale).log_prob(y).sum() / len(x)\n",
    "        \n",
    "        ood_q = torch.distributions.Normal(ood_mean_loc, ood_mean_scale)\n",
    "        ood_p = torch.distributions.Normal(y, ood_mean_std * torch.ones_like(ood_mean_scale))\n",
    "        \n",
    "        ncp_loss = torch.distributions.kl_divergence(ood_p, ood_q).sum() / len(x)\n",
    "        \n",
    "        w_q = torch.distributions.Normal(\n",
    "            self.mean_weight_loc, \n",
    "            self.eps + F.softplus(self.mean_weight_scale_p),\n",
    "        )\n",
    "        w_p = torch.distributions.Normal(\n",
    "            torch.zeros_like(self.mean_weight_loc), \n",
    "            self.prior_scale * torch.ones_like(self.mean_weight_scale_p),\n",
    "        )\n",
    "        \n",
    "        b_q = torch.distributions.Normal(\n",
    "            self.mean_bias_loc,\n",
    "            self.eps + F.softplus(self.mean_bias_scale_p),\n",
    "        )\n",
    "        b_p = torch.distributions.Normal(\n",
    "            torch.zeros_like(self.mean_bias_loc),\n",
    "            self.prior_scale * torch.ones_like(self.mean_bias_scale_p),\n",
    "        )\n",
    "        \n",
    "        bbb_loss = (\n",
    "            torch.distributions.kl_divergence(w_q, w_p).sum() / n_samples\n",
    "            + torch.distributions.kl_divergence(b_q, b_p).sum() / n_samples\n",
    "        )\n",
    "        \n",
    "        return nll + bbb_scale * bbb_loss + ncp_scale * ncp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.TensorDataset(\n",
    "    torch.as_tensor(data['train']['inputs'], dtype=torch.float),\n",
    "    torch.as_tensor(data['train']['targets'], dtype=torch.float),\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=20,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NCPEstimator(\n",
    "    transform=torch.nn.Sequential(\n",
    "        torch.nn.Linear(1, 30),\n",
    "        torch.nn.LeakyReLU(),\n",
    "        torch.nn.Linear(30, 30),\n",
    "        torch.nn.LeakyReLU(),\n",
    "    ),\n",
    "    transform_features=30,\n",
    "    out_features=1,\n",
    "    prior_scale=0.1,\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for loop, _ in Loop.over(range(1_000)):\n",
    "    for x, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(\n",
    "            x, y, \n",
    "            input_noise=0.1, \n",
    "            ood_mean_std=1.0, \n",
    "            n_samples=len(dataset), \n",
    "            ncp_scale=1e-2,\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(float(loss))\n",
    "        loop.print(f'{loop} {losses[-1]}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(target_loc, target_scale), (mean_loc, mean_scale) = t2n(model(torch.as_tensor(data['domain'], dtype=torch.float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0, c1, c2 = get_color_cycle(3)\n",
    "\n",
    "plt.plot(data['domain'][:, 0], mean_loc[:, 0], '.')\n",
    "plt.fill_between(\n",
    "    data['domain'][:, 0], \n",
    "    mean_loc[:, 0] - (target_scale[:, 0] ** 2.0 + mean_scale[:, 0] ** 2.0) ** 0.5,\n",
    "    mean_loc[:, 0] + (target_scale[:, 0] ** 2.0 + mean_scale[:, 0] ** 2.0) ** 0.5, \n",
    "    color=c0,\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "for label, c in [('train', c1), ('test', c2)]:\n",
    "    plt.plot(data[label]['inputs'][:, 0], data[label]['targets'][:, 0], '.', alpha=0.2, c=c, label=label)\n",
    "    \n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['domain'][:, 0], mean_scale[:, 0] ** 2.0)\n",
    "plt.plot(data['domain'][:, 0], target_scale[:, 0] ** 2.0)\n",
    "plt.plot(data['domain'][:, 0], target_scale[:, 0] ** 2.0 + mean_scale[:, 0] ** 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "misc-exp",
   "language": "python",
   "name": "misc-exp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
